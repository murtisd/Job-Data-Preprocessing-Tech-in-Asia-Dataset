# Data Cleaning and Preparation of Job Vacancy Dataset Scraped from Tech in Asia

## Project Description
This project aims to transform raw job vacancy data scraped from the **Tech in Asia** website into a structured, clean, and analysis-ready dataset.  
The scraping process was carried out using the **Web Scraper** Chrome extension to collect job postings in Indonesia through three stages between **November 2024 and January 2025**.

The data cleaning process included removing irrelevant columns, handling missing or inconsistent values, adjusting data types and formats, simplifying location names, and removing duplicate entries.  
Each record was also assigned a **unique Job ID** and manually labeled with a **job category** to enhance data usability.

The final cleaned dataset contains **628 valid rows and seven key attributes**, resulting in a high-quality dataset ready for **exploratory analysis** and **machine learning modeling**.

---

## Objective
The main objective of this project is to prepare and refine raw job vacancy data from Tech in Asia into a well-structured, clean dataset suitable for data analysis and machine learning tasks.  
This project also aims to establish a consistent and efficient preprocessing workflow for similar datasets in the future.

---

## Research Questions
- How can raw job vacancy data obtained through web scraping be effectively cleaned and structured for analysis?  
- What preprocessing steps are most critical to ensure high-quality and reliable datasets?  

---

## Process
1. Perform web scraping using the **Web Scraper** extension in Google Chrome.  
2. Merge data from three scraping sessions (Nov 2024 â€“ Jan 2025).  
3. Remove unnecessary columns.  
4. Handle missing and inconsistent data.  
5. Adjust data types and formatting.  
6. Simplify location names for consistency.  
7. Remove duplicate records.  
8. Add a **Job ID** and assign **job category labels**.  
9. Export the cleaned dataset for modeling and visualization purposes.  

---

## Dashboard
An **interactive dashboard** was created to visualize job distributions by category, location, and required skills.  
This dashboard helps users understand labor market trends and identify the most in-demand skills across industries.

ðŸ”— [View Dashboard on Google Looker Studio](https://lookerstudio.google.com/reporting/ed13a0fc-1e74-4fb4-9625-de6b65a46f88)

---

## Project Insights
- Most job postings are concentrated in major cities such as **Jakarta** and **Bandung**.  
- The most frequent skills required are related to **software development**, **data analysis**, and **project management**.  
- A well-executed data cleaning process significantly improves the reliability of analysis and the performance of subsequent machine learning models.  

---

## Conclusion
This project successfully transformed raw scraped job vacancy data into a clean and structured dataset with consistent formatting and labeling.  
The final dataset, containing **628 records**, is ready to be used for analysis and machine learning modeling.  
This process demonstrates the importance of careful data preparation as a foundation for generating accurate and meaningful insights in data-driven projects.
